1. 多线程的三个基本特性是什么？java是怎么实现原子性，有序性和一致性的呢？
Ø 原子性是指一个线程的操作是不能被其他线程打断，要么全部执行完毕，要么不执行,同一时间只有一个线程对一个变量进行操作。在多线程情况下，每个线程的执行结果不受其他线程的干扰，比如说多个线程同时对同一个共享成员变量n++100次，如果n初始值为0，n最后的值应该是100，所以说它们是互不干扰的，这就是传说的中的原子性。但n++并不是原子性的操作，要使用AtomicInteger保证原子性。
基本类型数据的访问大都是原子操作，long 和double类型的变量是64位，但是在32位JVM中，32位的JVM会将64位数据的读写操作分为2次32位的读写操作来进行，这就导致了long、double类型的变量在32位虚拟机中是非原子操作，数据有可能会被破坏，也就意味着多个线程在并发访问的时候是线程非安全的。

Ø 可见性是指某个线程修改了某一个共享变量的值，而其他线程是否可以看见该共享变量修改后的值。在单线程中肯定不会有这种问题，单线程读到的肯定都是最新的值，而在多线程编程中就不一定了。
每个线程都有自己的工作内存，线程先把共享变量的值从主内存读到工作内存，形成一个副本，当计算完后再把副本的值刷回主内存，从读取到最后刷回主内存这是一个过程，当还没刷回主内存的时候这时候对其他线程是不可见的，所以其他线程从主内存读到的值是修改之前的旧值。
像CPU的缓存优化、硬件优化、指令重排及对JVM编译器的优化，都会出现可见性的问题。

Ø 我们都知道程序是按代码顺序执行的，对于单线程来说确实是如此，但在多线程情况下就不是如此了。为了优化程序执行和提高CPU的处理性能，JVM和操作系统都会对指令进行重排，也就说前面的代码并不一定都会在后面的代码前面执行，即后面的代码可能会插到前面的代码之前执行，只要不影响当前线程的执行结果。所以，指令重排只会保证当前线程执行结果一致，但指令重排后势必会影响多线程的执行结果。
虽然重排序优化了性能，但也是会遵守一些规则的，并不能随便乱排序，只是重排序会影响多线程执行的结果

Ø 在java中提供了两个高级的字节码指令monitorenter和monitorexit，在Java中对应的Synchronized来保证代码块内的操作是原子的
Ø Java中的volatile关键字提供了一个功能，那就是被其修饰的变量在被修改后可以立即同步到主内存，被其修饰的变量在每次使用之前都从主内存刷新。因此，可以使用volatile来保证多线程操作时变量的可见性。
除了volatile，Java中的synchronized和ﬁnal两个关键字也可以实现可见性
Ø 在Java中，可以使用synchronized和volatile来保证多线程之间操作的有序性（编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序）。实现方式有所区别：
volatile关键字会禁止指令重排。synchronized关键字保证同一时刻只允许一条线程操作，单线程代码是按顺序执行的。


2. volatile的作用是什么？底层是怎么实现的？缓存的一致性协议是什么？有没有了解过内存屏障？
Ø volatile关键字保证了在多线程环境下,被修饰的变量在被修改后会马上同步到主存,这样该线程对这个变量的修改就是对所有其他线程可见的,其他线程能够马上读到这个修改后值.它能保证可见性、有序性，但不能保证原子性。

Ø volatile定义：
当对volatile变量执行写操作后，JMM会把工作内存中的最新变量值强制刷新到主内存
写操作会导致其他线程中的缓存无效。这样，其他线程使用缓存时，发现本地工作内存中此变量无效，便从主内存中获取，这样获取到的变量便是最新的值，实现了线程的可见性。
volatile是通过编译器在生成字节码时，在指令序列中添加“内存屏障”来禁止指令重排序的。

Ø 使用volatile以后,做了如下事情：
• 每次修改volatile变量都会同步到主存中
• 每次读取volatile变量的值都强制从主存读取最新的值(强制JVM不可优化volatile变量,如JVM优化后变量读取会使用cpu缓存而不从主存中读取)
• 线程 A 中写入 volatile 变量之前可见的变量, 在线程 B 中读取该 volatile 变量以后, 线程 B 对其他在 A 中的可见变量也可见. 换句话说, 写 volatile 类似于退出同步块, 而读取 volatile 类似于进入同步块。当然使用volatile的同时也会增加性能开销
• volatile并不能保证非原子性操作的多线程安全问题得到解决,volatile解决的是多线程间共享变量的可见性问题,而例如多线程的i++,++i,依然还是会存在多线程问题。多个线程同时读取这个共享变量的值，就算保证其他线程修改的可见性，也不能保证线程之间读取到同样的值然后相互覆盖对方的值的情况（读的是最新的值，但是写的时候覆盖掉对方的值了）。

Ø 因为CPU执行指令的速度远大于内存读写的速度，因此CPU会用高速缓存(Cache)来提高运行速度。当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中。如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致(我理解是数据脏读)的问题。
　   为了解决缓存不一致性问题，通常来说有以下2种解决方法：
　　1）通过在总线加LOCK#锁的方式
　　2）通过缓存一致性协议
由于第一种方式在锁住总线期间，其他CPU无法访问内存，导致效率低下。所以就出现了缓存一致性协议。最出名的就是Intel 的MESI协议，MESI协议保证了每个缓存中使用的共享变量的副本是一致的。它核心的思想是：当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。当共享变量用volatile修饰后就会帮我们在总线开启一个MESI缓存协议。同时会开启CPU总线嗅探机制(监听)，当其中一个线程将修改后的值传输到总线时，就会触发总线的监控告诉其他正在使用当前共享变量的线程使当前工作内存中的值失效。这个时候工作空间中的副本变量就没有了，从而需要重新去获取新的值。

Ø 内存屏障（Memory Barrier），又称内存栅栏，是一个CPU指令，通过内存屏障可以禁止特定类型处理器的重排序，从而让程序按我们预想的流程去执行。基本上它是一条这样的指令：
保证特定操作的执行顺序。
影响某些数据（或则是某条指令的执行结果）的内存可见性。

编译器和CPU能够重排序指令，保证最终相同的结果，尝试优化性能。插入一条Memory Barrier会告诉编译器和CPU：不管什么指令都不能和这条Memory Barrier指令重排序。
Memory Barrier所做的另外一件事是强制刷出各种CPU cache，如一个Write-Barrier（写入屏障）将刷出所有在Barrier之前写入 cache 的数据，因此，任何CPU上的线程都能读取到这些数据的最新版本。

硬件层面的“内存屏障”：
sfence：即写屏障(Store Barrier)，在写指令之后插入写屏障，能让写入缓存的最新数据写回到主内存，以保证写入的数据立刻对其他线程可见
lfence：即读屏障(Load Barrier)，在读指令前插入读屏障，可以让高速缓存中的数据失效，重新从主内存加载数据，以保证读取的是最新的数据。
mfence：即全能屏障(modify/mix Barrier )，兼具sfence和lfence的功能
lock 前缀：lock不是内存屏障，而是一种锁。执行时会锁住内存子系统来确保执行顺序，甚至跨多个CPU。

JMM层面的“内存屏障”：
LoadLoad屏障： 对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。
StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。
LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。
StoreLoad屏障： 对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。

JVM的实现会在volatile读写前后均加上内存屏障，在一定程度上保证有序性。如下所示：
LoadLoadBarrier
volatile 读操作
LoadStoreBarrier

StoreStoreBarrier
volatile 写操作
StoreLoadBarrier


Ø volatile在字节码层面，就是使用访问标志：ACC_VOLATILE来表示，供后续操作此变量时判断访问标志是否为ACC_VOLATILE，来决定是否遵循volatile的语义处理。

汇编层面：底层实现主要是通过汇编Lock前缀指令，它会锁定这块内存区域的缓存（缓存行锁定）并写回到主内存。总的来说就是Lock指令会将当前处理器缓存行数据立即写回到系统内存从而保证多线程数据缓存的时效性。这个写回内存的操作同时会引起在其它CPU里缓存了该内存地址的数据失效（MESI协议）。为了保证在从工作内存刷新回主内存这个阶段主内存数据的安全性，在store前会使用内存模型当中的lock操作来锁定当前主内存中的共享变量。当主内存变量在write操作后才会将当前lock释放掉，别的线程才能继续进来获取新的值

volatile是通过编译器在生成字节码时，在指令序列中添加“内存屏障”来禁止指令重排序的。同时lock前缀指令相当于一个内存屏障，它会告诉CPU和编译器先于这个命令的必须先执行，后于这个命令的必须后执行。内存屏障另一个作用是强制更新一次不同CPU的缓存。例如，一个写屏障会把这个屏障前写入的数据刷新到缓存，因为Memory Barrier会刷出cache中的所有先前的写入，这样任何试图读取该数据的线程将得到最新值，而不用考虑到底是被哪个cpu核心或者哪颗CPU执行的。

不同硬件实现内存屏障的方式不同，Java内存模型屏蔽了这种底层硬件平台的差异，由JVM来为不同的平台生成相应的机器码。
Java内存屏障主要有Load和Store两类：
对Load Barrier来说，在读指令前插入读屏障，可以让高速缓存中的数据失效，重新从主内存加载数据
对Store Barrier来说，在写指令之后插入写屏障，能让写入缓存的最新数据写回到主内存
为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。然而，对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能，为此，Java内存模型采取保守策略：
在每个volatile写操作的前面插入一个StoreStore屏障。
在每个volatile写操作的后面插入一个StoreLoad屏障。
在每个volatile读操作的后面插入一个LoadLoad屏障。
在每个volatile读操作的后面插入一个LoadStore屏障。


3. i++ 是线程安全的吗？
Ø 不是线程安全的，因为其非原子性操作。i++执行了多部操作, 从变量i中读取读取i的值 -> 值+1 -> 将+1后的值写回i中；
Ø volitale无法解决，因为volitale只是解决多线程间共享变量的可见性问题，无法解决非原子操作的线程同步问题，不能保证原子性；
Ø 对于 i++ 这种线程不安全问题的解决方案：
1、对 i++ 操作的方法加同步锁，同时只能有一个线程执行 i++ 操作；
2、使用支持原子性操作的类，如 java.util.concurrent.atomic.AtomicInteger，它使用的是 CAS 算法，效率优于第 1 种；
Ø 结论：
volatile解决了线程间共享变量的可见性问题
使用volatile会增加性能开销
volatile并不能解决线程同步问题
解决i++或者++i这样的线程同步问题需要使用synchronized或者AtomicXX系列的包装类,同时也会增加性能开销

4. java中的锁池和等待池
在java中，每个对象都有两个池，锁(monitor)池和等待池。
Ø 锁池:假设线程A已经拥有了某个对象(注意:不是类)的锁，而其它的线程想要调用这个对象的某个synchronized方法(或者synchronized块)，由于这些线程在进入对象的synchronized方法之前必须先获得该对象的锁的拥有权，但是该对象的锁目前正被线程A拥有，所以这些线程就进入了该对象的锁池中。 
Ø 等待池:假设一个线程A调用了某个对象的wait()方法，线程A就会释放该对象的锁(因为wait()方法必须出现在synchronized中，这样自然在执行wait()方法之前线程A就已经拥有了该对象的锁)，同时线程A就进入到了该对象的等待池中。如果另外的一个线程调用了相同对象的notifyAll()方法，那么处于该对象的等待池中的线程就会全部进入该对象的锁池中，准备争夺锁的拥有权。如果另外的一个线程调用了相同对象的notify()方法，那么仅仅有一个处于该对象的等待池中的线程(随机)会进入该对象的锁池.

5. synchronized的原理有了解？和reentryLock的区别是什么？锁升级是指什么？java锁的实现原理
Ø synchronized关键字同时保证原子性、可见性和有序性
synchronized是同步锁，同步块内的代码相当于同一时刻单线程执行，故不存在原子性和指令重排序的问题
synchronized关键字的语义JMM有两个规定，保证其实现内存可见性：
线程解锁前，必须把共享变量的最新值刷新到主内存中；
线程加锁前，将清空工作内存中共享变量的值，从主内存中重新取值。

6. 并发和并行的概念有了解？Java实现并发的方式有几种？
Ø 并发与并行是两个既相似但是却不相同的概念：
并发性：又称共行性，是指处理多个同时性活动的能力。并发是指一个处理器（CPU）同时处理多个任务。其实质是一个物理CPU（也可以是多个物理CPU）在若干个程序之间多路复用，并发性是对有限物理资源强制行使多用户共享以提高效率。
并行：指同时发生两个并发事件，具有并发的含义。并发不一定并行，也可以说并发事件之间不一定要同一时刻发生。并行指两个或两个以上事件或活动在同一时刻发生，在多道程序环境下，并行使多个程序同一时刻可在不同CPU上同时执行。
Ø 并发是逻辑上的同时发生（simultaneous），而并行是物理上的同时发生。打个比方：并发就像一个人（CPU）喂两个小孩（程序）吃饭，表面上是两个小孩在吃饭，实际是一个人在喂。并行就是两个人喂两个小孩子吃饭。

并发、并行和多线程的关系：并行需要两个或两个以上的线程跑在不同的处理器上，并发可以跑在一个处理器上通过时间片进行切换。
